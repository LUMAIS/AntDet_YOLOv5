{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFNC03jtZsu5"
      },
      "source": [
        "# YOLOv5 training tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq3-j3aAjcla"
      },
      "source": [
        "## Step 0: Pull the YOLO repository and the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNHysfPTZo9f",
        "outputId": "7e5364d3-5b9d-4ca8-af2d-e6ef4530a531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.12.1+cu113 (Tesla T4)\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/PikachuDeveloper/yolo_data.git\n",
        "# %cd yolo_data\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gGVvG-gKrjtO"
      },
      "source": [
        "### Step 0.1: Transform the existing annotations\n",
        "\n",
        "To create a folder \"datasets/labels\" inside the project folder.\n",
        "Transform the Labelbox annotations (can be downloaded from [Lumais Cloud](https://cloud.lumais.com/s/3XyiFZzosC3wx6w) or LabelBox) into the YOLOv5 format by running the following row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY68it_nrpbA",
        "outputId": "e9f0fc59-023c-4d63-c3a6-332534931762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/yolo_data/'\n",
            "/content/yolov5\n",
            "python3: can't open file 'lbxTorch.py': [Errno 2] No such file or directory\n",
            "python3: can't open file 'lbxTorch.py': [Errno 2] No such file or directory\n",
            "python3: can't open file 'lbxTorch.py': [Errno 2] No such file or directory\n",
            "python3: can't open file 'lbxTorch.py': [Errno 2] No such file or directory\n",
            "python3: can't open file 'lbxTorch.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%cd /content/  # yolo_data/\n",
        "!python lbxTorch.py -o datasets/labels -json-path 'Cflo_troph_count_masked_5-30_6-03-rand1.json' -s '1920x1080' -pname 'Cflo_troph_count_masked_5-30_6-03-rand1'\n",
        "!python lbxTorch.py -o datasets/labels -json-path 'Cflo_troph_count_3-38_3-52.json' -s '1920x1080' -pname 'Cflo_troph_count_3-38_3-52'\n",
        "!python lbxTorch.py -o datasets/labels -json-path '523cropped_s50p_f10.json' -s '1194x1194' -pname '523cropped_s50p_f10'\n",
        "!python lbxTorch.py -o datasets/labels -json-path '6.json' -s '2496x2200' -pname '6'\n",
        "!python lbxTorch.py -o datasets/labels -json-path '100testimages_s25p_f10.json' -s '1000x1000' -pname '100testimages_s25p_f10'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVIKHGF5ss4m"
      },
      "source": [
        " To do the same, for your own data follow the format:\n",
        " ```\n",
        " !python lbxTorch.py -o datasets/labels -json-path <annotation_file_name> -s <frame_size_WxH> -pname <res_annontations_name>\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN-nCf3otnda"
      },
      "source": [
        "### Step 0.2: Transform the existing videos into the frames\n",
        "\n",
        "The following code splits videos into the images with the name in format `<vidname>_<framenumber>.jpg`.\n",
        "\n",
        "To perform the same for your video, add your video name into the downLoad dictionary as it is written in a commented instruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ukmcyJYXteH8",
        "outputId": "e334939b-6e20-467d-be24-4f8353dce838"
      },
      "outputs": [
        {
          "ename": "error",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-06470f822730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     annpath = os.path.join(path, img_dir,\n\u001b[1;32m     24\u001b[0m                            '{}_{}.jpg'.format(filename.split('.')[0], i))\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mvid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgcodecs/src/loadsave.cpp:801: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n"
          ]
        }
      ],
      "source": [
        "import os, cv2\n",
        "\n",
        "downLoad = {\n",
        "    # vidname: number of annotated frames or -1 if all frames were annoted\n",
        "    # The videos can be downloaded from [Lumais Cloud](https://cloud.lumais.com/s/nKHYDLDzWKnLgp3) or Labelbox\n",
        "    'Cflo_troph_count_masked_5-30_6-03-rand1.mp4': -1,\n",
        "    'Cflo_troph_count_3-38_3-52.mp4': -1,\n",
        "    '523cropped_s50p_f10.mp4': 111,\n",
        "    '6.mp4': 175,\n",
        "    '100testimages_s25p_f10.mp4': -1\n",
        "    }\n",
        "\n",
        "img_dir = os.path.join('datasets', 'images')\n",
        "if not os.path.isdir(img_dir):\n",
        "  os.makedirs(img_dir)\n",
        "\n",
        "path = os.getcwd()\n",
        "for filename, num in downLoad.items():\n",
        "  vidpath = os.path.join(path, filename)\n",
        "  vid = cv2.VideoCapture(vidpath)\n",
        "  num = int(vid.get(cv2.CAP_PROP_FRAME_COUNT)) if num == -1 else num\n",
        "  for i in range(1, num + 1):\n",
        "    _, frame = vid.read()\n",
        "    annpath = os.path.join(path, img_dir,\n",
        "                           '{}_{}.jpg'.format(filename.split('.')[0], i))\n",
        "    cv2.imwrite(annpath, frame)\n",
        "    \n",
        "  vid.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0xXZ3AW43bs"
      },
      "source": [
        "### Step 0.3: Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCSjCIv-42ar"
      },
      "outputs": [],
      "source": [
        "%cd /content/  # yolo_data/\n",
        "\n",
        "from random import shuffle\n",
        "from shutil import move\n",
        "from glob import glob\n",
        "\n",
        "start = os.getcwd()\n",
        "imgs = os.path.join(os.getcwd(), img_dir)\n",
        "labs = os.getcwd() + '/datasets/labels'\n",
        "img_path = 'images'\n",
        "lbs_path = 'labels'\n",
        "test_folder = 'val'\n",
        "train_folder = 'train'\n",
        "\n",
        "test_percent = 0.2\n",
        "try:\n",
        "  os.remove(\n",
        "      os.path.join(imgs + '/523cropped_s50p_f10_1.jpg')\n",
        "      )\n",
        "  os.remove(\n",
        "      os.path.join(imgs + '/523cropped_s50p_f10_2.jpg')\n",
        "      )\n",
        "except: pass\n",
        "os.chdir(imgs)\n",
        "files = glob(\n",
        "    \"*.jpg\"\n",
        "    )\n",
        "os.chdir(start)\n",
        "shuffle(files)\n",
        "\n",
        "for folder in (test_folder, train_folder):\n",
        "  end = os.path.join(start, folder)\n",
        "  images =  os.path.join(end, img_path)\n",
        "  labels = os.path.join(end, lbs_path)\n",
        "\n",
        "  if not os.path.isdir(images):\n",
        "    os.makedirs(images)\n",
        "  if not os.path.isdir(labels):\n",
        "    os.makedirs(labels)\n",
        "  \n",
        "  if folder == test_folder:\n",
        "    filenames = files[: int(test_percent * len(files))]\n",
        "  else:\n",
        "    filenames = files[int(test_percent * len(files)):]\n",
        "\n",
        "  for file in filenames:\n",
        "    try:\n",
        "      os.chdir(labels)\n",
        "      move(os.path.join(labs, file.rstrip('.jpg') + '.txt'), os.path.join(labels, file.rstrip('.jpg') + '.txt'))\n",
        "      os.chdir(images)\n",
        "      move(os.path.join(imgs, file), os.path.join(images, file))\n",
        "    except Exception as e: print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cixp9yto6T7Q"
      },
      "outputs": [],
      "source": [
        "if len(glob('/content/yolo_data/val/images/*')) == len(glob('/content/yolo_data/val/labels/*')) > 0:\n",
        "  print(\"Correct length in test data\", len(glob('/content/yolo_data/val/images/*')))\n",
        "if len(glob('/content/yolo_data/train/images/*')) == len(glob('/content/yolo_data/train/labels/*')) > 0:\n",
        "  print(\"Correct length in train data\", len(glob('/content/yolo_data/train/images/*')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWL8NwKXA47Y"
      },
      "source": [
        "## Step 1: Preparing for training\n",
        "\n",
        "Open `yolo_data/yolov5/utils/metrics.py` and replace `w = [0.0, 0.0, 0.1, 0.9]` on 19th row of the file with `[0.0, 0.1, 0.05, 0.85]` or `[0.0, 0.2, 0.05, 0.75]` to raise the valuability of the recall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEfg2jpn8umX"
      },
      "source": [
        "## Step 2: Training\n",
        "### Step 2.1: First training using yolov5\n",
        "\n",
        "To enlarge the number of epochs tune the `epochs` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_UbGl_DRHZ4",
        "outputId": "14060420-aab9-40ea-dd22-155e50fc925c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=../data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=4, imgsz=1024, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v6.2-183-gc98128f Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs in Weights & Biases\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 113MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 132MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     35061  models.yolo.Detect                      [8, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7041205 parameters, 7041205 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/yolo_data/train' images and labels...0 found, 1 missing, 0 empty, 0 corrupt: 100% 1/1 [00:00<00:00, 734.81it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ No labels found in /content/yolo_data/train.cache. See https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolo_data/train.cache\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 630, in <module>\n",
            "    main(opt)\n",
            "  File \"train.py\", line 526, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"train.py\", line 200, in train\n",
            "    shuffle=True)\n",
            "  File \"/content/yolo_data/yolov5/utils/dataloaders.py\", line 135, in create_dataloader\n",
            "    prefix=prefix)\n",
            "  File \"/content/yolo_data/yolov5/utils/dataloaders.py\", line 497, in __init__\n",
            "    assert nf > 0 or not augment, f'{prefix}No labels found in {cache_path}, can not start training. {HELP_URL}'\n",
            "AssertionError: \u001b[34m\u001b[1mtrain: \u001b[0mNo labels found in /content/yolo_data/train.cache, can not start training. See https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data\n"
          ]
        }
      ],
      "source": [
        "# %cd yolov5\n",
        "!python train.py --img 1024 --batch 4 --epochs 10 --data ../data.yaml --weights yolov5s.pt  \n",
        "#--hyp ../hyp_evolve1.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGuYN4yZbpqf"
      },
      "source": [
        "### Step 2.2: Training using new weights and hyperparameters\n",
        "\n",
        "The weights are usually saved in the folder `yolov5/runs/train/exp<INDEX>/weights/best.pt` and hyperparameters are saved in `yolov5/runs/evolve/exp/hyp_evolve.yaml`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2TEdyGIbpqg"
      },
      "outputs": [],
      "source": [
        "!python train.py --img 1024 --batch 4 --epochs 10 --data ../data.yaml --weights runs/train/exp<INDEX>/weights/best.pt  --hyp runs/evolve/exp/hyp_evolve.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0iOZkIe2_5e"
      },
      "source": [
        "## Step 3: Hyperparameter evolution\n",
        "\n",
        "Updates hyperparameters to speed up the training.\n",
        "\n",
        "If you want to evolve your current hyperparameters, add a parameter `--hyp` and a path to the hyperparameters path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Necysbrk2_5o",
        "outputId": "ce57817e-60f2-4462-c2a6-e82f9e620dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=data/coco128.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=5, batch_size=4, imgsz=1024, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=5, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v6.2-183-gc98128f Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=3\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs in Weights & Biases\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\n",
            "Dataset not found ⚠️, missing paths ['/content/datasets/coco128/images/train2017']\n",
            "Downloading https://ultralytics.com/assets/coco128.zip to coco128.zip...\n",
            "100% 6.66M/6.66M [00:00<00:00, 10.1MB/s]\n",
            "Dataset download success ✅ (2.2s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 99.9MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 290MB/s]\n",
            "\n",
            "Overriding model.yaml anchors with anchors=3\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
            "\n",
            "Transferred 348/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/datasets/coco128/labels/train2017' images and labels...126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<00:00, 1864.81it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco128/labels/train2017.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/coco128/labels/train2017.cache' images and labels... 126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.14 anchors/target, 0.041 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ⚠️ Extremely small objects found: 3 of 929 labels are <3 pixels in size\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 929 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.6588: 100% 1000/1000 [00:01<00:00, 878.89it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9839 best possible recall, 3.78 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=1024, metric_all=0.264/0.662-mean/best, past_thr=0.475-mean: 24,33, 67,43, 90,109, 219,136, 155,276, 224,488, 478,386, 585,594, 976,715\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n",
            "Plotting labels to runs/evolve/exp/labels.jpg... \n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/evolve/exp\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/4       2.4G     0.1169    0.09222    0.03991         69       1024: 100% 32/32 [00:09<00:00,  3.47it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/4       2.4G     0.1064    0.06552    0.03626         63       1024: 100% 32/32 [00:07<00:00,  4.12it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/4       2.4G    0.09203     0.0726    0.03546         58       1024: 100% 32/32 [00:07<00:00,  4.39it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/4       2.4G    0.08216    0.07582    0.03568         66       1024: 100% 32/32 [00:07<00:00,  4.35it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        4/4       2.4G    0.07173    0.08429    0.03118         37       1024: 100% 32/32 [00:07<00:00,  4.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 16/16 [00:02<00:00,  5.93it/s]\n",
            "                   all        128        929      0.386      0.386      0.377      0.143\n",
            "\n",
            "5 epochs completed in 0.012 hours.\n",
            "Results saved to \u001b[1mruns/evolve/exp\u001b[0m\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m1 generations finished, current result:\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m             0.38574,              0.38646,              0.37651,              0.14293,             0.068744,             0.061796,             0.012933,                 0.01,                 0.01,                0.937,               0.0005,                    3,                  0.8,                  0.1,                 0.05,                  0.5,                    1,                    1,                    1,                  0.2,                    4,                    0,                0.015,                  0.7,                  0.4,                    0,                  0.1,                  0.5,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    3\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01003, momentum=0.93608, weight_decay=0.0005, warmup_epochs=3.00731, warmup_momentum=0.8, warmup_bias_lr=0.09974, box=0.04988, cls=0.5, cls_pw=1.0, obj=0.99354, obj_pw=1.0, iou_t=0.2, anchor_t=3.97156, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.69537, hsv_v=0.40335, degrees=0.0, translate=0.09923, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.97602\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs in Weights & Biases\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "Overriding model.yaml anchors with anchors=2.97602\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
            "\n",
            "Transferred 348/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/datasets/coco128/labels/train2017.cache' images and labels... 126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/coco128/labels/train2017.cache' images and labels... 126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.13 anchors/target, 0.038 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ⚠️ Extremely small objects found: 3 of 929 labels are <3 pixels in size\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 929 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.6593: 100% 1000/1000 [00:01<00:00, 823.02it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9839 best possible recall, 3.84 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=1024, metric_all=0.268/0.662-mean/best, past_thr=0.477-mean: 24,33, 65,42, 87,101, 209,139, 157,272, 226,487, 398,399, 577,576, 892,682\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n",
            "Plotting labels to runs/evolve/exp/labels.jpg... \n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/evolve/exp\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/4       2.4G     0.1157    0.09256    0.03901         69       1024: 100% 32/32 [00:07<00:00,  4.08it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/4       2.4G     0.1048    0.06674    0.03636         63       1024: 100% 32/32 [00:07<00:00,  4.14it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/4       2.4G    0.08968    0.07434    0.03469         58       1024: 100% 32/32 [00:07<00:00,  4.21it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/4       2.4G    0.08061    0.07722    0.03562         66       1024: 100% 32/32 [00:07<00:00,  4.17it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        4/4       2.4G    0.07024    0.08508    0.03087         37       1024: 100% 32/32 [00:07<00:00,  4.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 16/16 [00:02<00:00,  6.28it/s]\n",
            "                   all        128        929      0.376      0.352      0.329      0.126\n",
            "\n",
            "5 epochs completed in 0.011 hours.\n",
            "Results saved to \u001b[1mruns/evolve/exp\u001b[0m\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m2 generations finished, current result:\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m             0.37603,              0.35228,              0.32878,              0.12624,              0.06825,             0.062452,             0.017634,                 0.01,              0.01003,              0.93608,               0.0005,               3.0073,                  0.8,              0.09974,              0.04988,                  0.5,                    1,              0.99354,                    1,                  0.2,               3.9716,                    0,                0.015,              0.69537,              0.40335,                    0,              0.09923,              0.49617,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                2.976\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00992, lrf=0.01007, momentum=0.94038, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0984, box=0.05, cls=0.49475, cls_pw=0.97707, obj=0.99577, obj_pw=1.0, iou_t=0.2, anchor_t=4.06031, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.69914, hsv_v=0.39925, degrees=0.0, translate=0.09995, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.99175, mixup=0.0, copy_paste=0.0, anchors=3.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs in Weights & Biases\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "Overriding model.yaml anchors with anchors=3.0\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
            "\n",
            "Transferred 348/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00992) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/datasets/coco128/labels/train2017.cache' images and labels... 126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/coco128/labels/train2017.cache' images and labels... 126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.14 anchors/target, 0.042 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ⚠️ Extremely small objects found: 3 of 929 labels are <3 pixels in size\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 929 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.6580: 100% 1000/1000 [00:01<00:00, 824.37it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9839 best possible recall, 3.88 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=1024, metric_all=0.264/0.661-mean/best, past_thr=0.468-mean: 28,33, 84,50, 64,130, 180,134, 157,283, 229,477, 472,351, 580,614, 999,673\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n",
            "Plotting labels to runs/evolve/exp/labels.jpg... \n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/evolve/exp\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/4      2.41G     0.1174    0.09401    0.03877         33       1024: 100% 32/32 [00:08<00:00,  3.96it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/4      2.41G     0.1076    0.06452     0.0383         63       1024: 100% 32/32 [00:07<00:00,  4.29it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/4      2.41G    0.09283     0.0719    0.03435         42       1024: 100% 32/32 [00:07<00:00,  4.33it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/4      2.41G    0.08145    0.07661    0.03509         66       1024: 100% 32/32 [00:08<00:00,  3.85it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        4/4      2.41G    0.07215     0.0879    0.02745         65       1024: 100% 32/32 [00:08<00:00,  3.66it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 16/16 [00:02<00:00,  5.91it/s]\n",
            "                   all        128        929      0.351      0.353      0.306      0.124\n",
            "\n",
            "5 epochs completed in 0.012 hours.\n",
            "Results saved to \u001b[1mruns/evolve/exp\u001b[0m\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m3 generations finished, current result:\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m             0.35065,              0.35338,              0.30647,              0.12415,             0.068709,             0.061678,             0.016547,              0.00992,              0.01007,              0.94038,               0.0005,                    3,                  0.8,               0.0984,                 0.05,              0.49475,              0.97707,              0.99577,                    1,                  0.2,               4.0603,                    0,                0.015,              0.69914,              0.39925,                    0,              0.09995,                  0.5,                    0,                    0,                    0,                  0.5,              0.99175,                    0,                    0,                    3\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0097, lrf=0.01075, momentum=0.9488, weight_decay=0.00046, warmup_epochs=3.0073, warmup_momentum=0.85786, warmup_bias_lr=0.10085, box=0.04558, cls=0.5, cls_pw=1.0, obj=0.98711, obj_pw=0.97833, iou_t=0.2, anchor_t=4.20248, fl_gamma=0.0, hsv_h=0.01503, hsv_s=0.70305, hsv_v=0.41966, degrees=0.0, translate=0.09768, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.99648\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs in Weights & Biases\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "Overriding model.yaml anchors with anchors=2.99648\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
            "\n",
            "Transferred 348/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0097) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00046), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/datasets/coco128/labels/train2017.cache' images and labels... 126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/coco128/labels/train2017.cache' images and labels... 126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.17 anchors/target, 0.048 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ⚠️ Extremely small objects found: 3 of 929 labels are <3 pixels in size\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 929 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.6693: 100% 1000/1000 [00:01<00:00, 735.12it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.24: 0.9946 best possible recall, 3.88 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=1024, metric_all=0.261/0.670-mean/best, past_thr=0.467-mean: 21,21, 35,44, 81,91, 205,138, 144,273, 219,448, 536,362, 593,630, 928,735\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n",
            "Plotting labels to runs/evolve/exp/labels.jpg... \n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/evolve/exp\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/4       2.4G     0.1025     0.0914    0.03902         69       1024: 100% 32/32 [00:07<00:00,  4.05it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/4      2.41G    0.09123    0.06887    0.03557         63       1024: 100% 32/32 [00:08<00:00,  3.76it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/4      2.41G    0.07928    0.07258     0.0332         58       1024: 100% 32/32 [00:07<00:00,  4.03it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/4      2.41G    0.07074    0.07638    0.03372         66       1024: 100% 32/32 [00:08<00:00,  3.78it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        4/4      2.41G    0.06275    0.08575    0.03087         37       1024: 100% 32/32 [00:07<00:00,  4.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 16/16 [00:02<00:00,  5.96it/s]\n",
            "                   all        128        929      0.344      0.453      0.404      0.167\n",
            "\n",
            "5 epochs completed in 0.012 hours.\n",
            "Results saved to \u001b[1mruns/evolve/exp\u001b[0m\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m4 generations finished, current result:\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m             0.34444,              0.45322,              0.40438,              0.16746,             0.060958,             0.063594,             0.013335,               0.0097,              0.01075,               0.9488,              0.00046,               3.0073,              0.85786,              0.10085,              0.04558,                  0.5,                    1,              0.98711,              0.97833,                  0.2,               4.2025,                    0,              0.01503,              0.70305,              0.41966,                    0,              0.09768,              0.49617,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.9965\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.96, weight_decay=0.00066, warmup_epochs=3.0073, warmup_momentum=0.8, warmup_bias_lr=0.10644, box=0.04227, cls=0.42032, cls_pw=1.01353, obj=0.99354, obj_pw=1.06283, iou_t=0.2, anchor_t=2.98643, fl_gamma=0.0, hsv_h=0.02119, hsv_s=0.70512, hsv_v=0.46452, degrees=0.0, translate=0.06704, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=5.09967\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs in Weights & Biases\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "Overriding model.yaml anchors with anchors=5.09967\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    382075  models.yolo.Detect                      [80, [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7388219 parameters, 7388219 gradients, 17.1 GFLOPs\n",
            "\n",
            "Transferred 342/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00066), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/datasets/coco128/labels/train2017.cache' images and labels... 126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/coco128/labels/train2017.cache' images and labels... 126 found, 2 missing, 0 empty, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.46 anchors/target, 0.083 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ⚠️ Extremely small objects found: 3 of 929 labels are <3 pixels in size\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 15 anchors on 929 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7256: 100% 1000/1000 [00:01<00:00, 778.80it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.33: 0.9860 best possible recall, 4.59 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=15, img_size=1024, metric_all=0.260/0.729-mean/best, past_thr=0.531-mean: 17,18, 27,43, 66,36, 41,95, 103,107, 70,229, 212,127, 172,237, 176,368, 372,258, 230,514, 423,427, 713,381, 438,811, 915,736\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n",
            "Plotting labels to runs/evolve/exp/labels.jpg... \n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/evolve/exp\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/4      2.35G    0.09481    0.09192    0.09557         70       1024: 100% 32/32 [00:08<00:00,  3.96it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/4      2.36G    0.08827    0.07572    0.09375         62       1024: 100% 32/32 [00:08<00:00,  3.95it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/4      2.36G    0.07995    0.07211    0.09043         61       1024: 100% 32/32 [00:07<00:00,  4.13it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/4      2.36G    0.07578    0.06944    0.08811         66       1024: 100% 32/32 [00:07<00:00,  4.16it/s]\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        4/4      2.36G    0.06991    0.07685    0.08506         37       1024: 100% 32/32 [00:07<00:00,  4.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 16/16 [00:02<00:00,  7.32it/s]\n",
            "                   all        128        929    0.00548     0.0338    0.00619    0.00153\n",
            "\n",
            "5 epochs completed in 0.012 hours.\n",
            "Results saved to \u001b[1mruns/evolve/exp\u001b[0m\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m5 generations finished, current result:\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
            "\u001b[34m\u001b[1mevolve: \u001b[0m           0.0054764,             0.033813,             0.006189,            0.0015304,             0.068979,             0.050733,             0.082998,                 0.01,                 0.01,                 0.96,              0.00066,               3.0073,                  0.8,              0.10644,              0.04227,              0.42032,               1.0135,              0.99354,               1.0628,                  0.2,               2.9864,                    0,              0.02119,              0.70512,              0.46452,                    0,              0.06704,              0.49617,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               5.0997\n",
            "\n",
            "\n",
            "Best results from row 3 of runs/evolve/exp/evolve.csv:\n",
            "            lr0: 0.0097\n",
            "            lrf: 0.0107\n",
            "       momentum: 0.949\n",
            "   weight_decay: 0.00046\n",
            "  warmup_epochs: 3.01\n",
            "warmup_momentum: 0.858\n",
            " warmup_bias_lr: 0.101\n",
            "            box: 0.0456\n",
            "            cls: 0.5\n",
            "         cls_pw: 1\n",
            "            obj: 0.987\n",
            "         obj_pw: 0.978\n",
            "          iou_t: 0.2\n",
            "       anchor_t: 4.2\n",
            "       fl_gamma: 0\n",
            "          hsv_h: 0.015\n",
            "          hsv_s: 0.703\n",
            "          hsv_v: 0.42\n",
            "        degrees: 0\n",
            "      translate: 0.0977\n",
            "          scale: 0.496\n",
            "          shear: 0\n",
            "    perspective: 0\n",
            "         flipud: 0\n",
            "         fliplr: 0.5\n",
            "         mosaic: 1\n",
            "          mixup: 0\n",
            "     copy_paste: 0\n",
            "        anchors: 3\n",
            "Saved runs/evolve/exp/evolve.png\n",
            "Hyperparameter evolution finished 5 generations\n",
            "Results saved to \u001b[1mruns/evolve/exp\u001b[0m\n",
            "Usage example: $ python train.py --hyp runs/evolve/exp/hyp_evolve.yaml\n"
          ]
        }
      ],
      "source": [
        "!python train.py --img 1024 --batch 4 --epochs 5 --data ../data.yaml --weights <weights_path> --evolve 5\n",
        "# --hyp yolov5/runs/evolve/exp/hyp_evolve.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddBrQZ-m5U08"
      },
      "source": [
        "## Step 4: Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRMOch-D5U1H"
      },
      "outputs": [],
      "source": [
        "!python detect.py --save-txt --weights <weights>.pt --source <test_folder> --line-thickness 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-66KP8uD7lcD",
        "outputId": "4320d647-9816-4674-d1e8-82f757efbbb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=../data.yaml, weights=['runs/train/exp9/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=True, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 🚀 v6.2-182-g1158a50 Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7031701 parameters, 0 gradients, 15.8 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolo_data/val/labels.cache' images and labels... 2 found, 0 missing, 0 empty, 0 corrupt: 100% 2/2 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  2.85it/s]\n",
            "                   all          2        184          0          0          0          0\n",
            "Speed: 0.3ms pre-process, 20.3ms inference, 47.0ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n",
            "2 labels saved to runs/val/exp/labels\n"
          ]
        }
      ],
      "source": [
        "!python val.py --save-txt --weights <weights>.pt --data ../data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isSDidxfAjb8"
      },
      "source": [
        "## Warning!\n",
        "If something went wrong, you can remove the folders, but be careful!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDhVJJQl3D9N"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "\n",
        "# shutil.rmtree('/content/yolo_data/datasets/')\n",
        "# shutil.rmtree('/content/yolo_data/val/')\n",
        "# shutil.rmtree('/content/yolo_data/train/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gb0s5DN-ckH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gGVvG-gKrjtO",
        "Y0iOZkIe2_5e",
        "ddBrQZ-m5U08"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
